基于您的描述，我来设计一个整合多模态模型和纯文本LLM的后端通信逻辑，同时结合您已有的文档处理服务。

## 系统架构设计

### 服务组件

1. **用户请求处理服务**
   - 接收用户请求
   - 判断请求类型（纯文本/含附件）
   - 路由到相应的处理流程

2. **文档处理服务**
   - PDF转图像服务 (pdftoimages)
   - 图像转base64服务 (imagetobase64)
   - 文档切片和处理

3. **模型推理服务**
   - 多模态模型服务（支持base64图像+文本）
   - 纯文本LLM服务
   - 模型选择逻辑

4. **会话管理服务**
   - 维护对话上下文
   - 存储对话历史
   - 管理用户会话状态

### 通信流程

#### 纯文本问答流程：
1. 客户端发送文本请求
2. 用户请求处理服务接收并判断为纯文本请求
3. 直接路由到纯文本LLM服务
4. LLM生成响应
5. 响应通过会话管理服务回传给用户

#### 含PDF附件的问答流程：
1. 客户端上传PDF文件并发送问题
2. 文件上传到临时存储
3. PDF转图像服务将PDF转换为图像文件集
4. 图像转base64服务将图像转换为base64编码
5. 组装多模态请求（base64图像+文本问题）
6. 发送到多模态模型服务生成回答
7. 响应通过会话管理服务回传给用户

#### 含图片附件的问答流程：
1. 客户端上传图片文件并发送问题
2. 图像转base64服务将图像转换为base64编码
3. 组装多模态请求（base64图像+文本问题）
4. 发送到多模态模型服务生成回答
5. 响应通过会话管理服务回传给用户

## 具体实现方案

### 请求判断与路由

```python
def process_request(request):
    if has_attachment(request):
        attachment = request.attachment
        if is_pdf(attachment):
            return process_pdf_request(attachment, request.text)
        elif is_image(attachment):
            return process_image_request(attachment, request.text)
    else:
        return process_text_request(request.text)
```

### 文档处理实现

```python
def process_pdf_request(pdf_file, question):
    # 存储PDF
    pdf_path = save_temp_file(pdf_file)
    
    # PDF转图像
    image_paths = pdf_to_images(pdf_path)
    
    # 图像转base64
    base64_images = []
    for img_path in image_paths:
        base64_str = image_to_base64(img_path)
        base64_images.append(base64_str)
    
    # 调用多模态模型
    return call_multimodal_model(base64_images, question)

def process_image_request(image_file, question):
    # 存储图像
    image_path = save_temp_file(image_file)
    
    # 图像转base64
    base64_str = image_to_base64(image_path)
    
    # 调用多模态模型
    return call_multimodal_model([base64_str], question)
```

### 模型调用接口

```python
def call_text_llm(text_input, context=None):
    payload = {
        "prompt": text_input,
        "context": context or [],
        "max_tokens": 1000
    }
    response = requests.post(TEXT_LLM_ENDPOINT, json=payload)
    return response.json()

def call_multimodal_model(base64_images, text_input, context=None):
    payload = {
        "prompt": text_input,
        "images": base64_images,
        "context": context or [],
        "max_tokens": 1000
    }
    response = requests.post(MULTIMODAL_ENDPOINT, json=payload)
    return response.json()
```

## 数据结构设计

### API请求结构

```json
// 纯文本请求
{
    "session_id": "uuid-1234",
    "message": {
        "type": "text",
        "content": "什么是人工智能？"
    }
}

// 含图像的请求
{
    "session_id": "uuid-1234",
    "message": {
        "type": "multimodal",
        "content": "这张图片里有什么？",
        "attachment": {
            "type": "image",
            "file": "base64_encoded_binary_or_file_upload"
        }
    }
}

// 含PDF的请求
{
    "session_id": "uuid-1234",
    "message": {
        "type": "multimodal",
        "content": "总结这份文档",
        "attachment": {
            "type": "pdf",
            "file": "base64_encoded_binary_or_file_upload"
        }
    }
}
```

## 优化策略

1. **异步处理**
   - 使用异步任务队列处理文档转换
   - 支持长时间运行的PDF处理任务

2. **缓存机制**
   - 缓存已处理的文档和图像
   - 缓存常见问题的响应

3. **模型选择**
   - 智能路由到最适合的模型
   - 对于长文档可能分段处理后合并结果

4. **错误处理**
   - 文档处理失败自动降级到纯文本处理
   - 模型调用超时重试机制

5. **资源管理**
   - 临时文件的自动清理策略
   - 大型PDF的分批处理机制

这个设计充分利用了您现有的组件（多模态模型、纯文本LLM、PDF转图像服务和图像转base64服务），并提供了一个统一的通信流程来处理不同类型的用户请求。